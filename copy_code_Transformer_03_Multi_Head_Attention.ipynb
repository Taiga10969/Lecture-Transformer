{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3oY4x8yWp80ZsieUoOIqG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Taiga10969/Lecture-Transformer/blob/main/copy_code_Transformer_03_Multi_Head_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 写経して理解するTransformer_03：Multi-Head Attention\n",
        "\n",
        "Multi-Head Attentionは，Transformerモデルの重要な構成要素の1つである．<br>\n",
        "Attention機構は，モデルが文字列を構成する各トークンに対して重要度の重み付けを行い，その情報を利用して出力を生成するために使用される．<br>\n",
        "Multi-Head Attentionは，単一のAttentionヘッドではなく，複数のAttentionヘッドを使ってこの処理を行う．<br>\n",
        "<br>\n",
        "### **Multi-Head Attentionの概要**<br>\n",
        "\n",
        "**Query，Key，Valueの射影**<br>\n",
        "Multi-Head Attentionは，3つの異なるLinear層を持っており，元の入力の特徴量空間を異なる部分空間に射影する．<br>\n",
        "具体的には，Attention機構で用いられるQuery，Key，Valueの3つに射影する．<br>\n",
        "<br>\n",
        "**Attentionスコアの計算**<br>\n",
        "各Attentionヘッドでは，QueryとKeyの内積を計算してAttentionスコアを生成する．<br>\n",
        "これにより，各Queryに対してKeyの各要素との関連度を表すスコアを獲得する．<br>\n",
        "<br>\n",
        "**重み付きのValueの取得**<br>\n",
        "Attentionスコアはsoftmax関数を使用して正規化され，各Valueに重みが付けされる．<br>\n",
        "この重みは，QueryとKeyの関連度を表し，その関連度に基づいてValueに重み付けする．<br>\n",
        "<br>\n",
        "**Multi-Headの結合**<br>\n",
        "すべてのAttentionヘッドがこれらの重み付きValueを生成し，最後に各ヘッドで生成されたベクトルを結合する．<br>\n",
        "これにより，多様な観点からの情報が網羅され，より豊かな表現が得られる．<br>\n",
        "<br>\n",
        "**最終的な出力の生成**<br>\n",
        "結合された重み付きValueは，再度線形射影を通して最終的な出力を生成する．<br>\n",
        "<br>\n",
        "Multi-Head Attentionの利点は，異なる注意の表現を複数使用するところである．<br>\n",
        "各ヘッドは異なる部分空間に射影されるため，異なる種類の情報をキャプチャし，モデルがより広範な文脈を捉えるのに役立つ．<br>\n",
        "これにより，モデルの表現力が向上し，複雑な関係をモデル化できる．"
      ],
      "metadata": {
        "id": "Nvl91jgzqf7Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 必要ライブラリのインポート"
      ],
      "metadata": {
        "id": "caTBFVoItExo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "xCffw1dJoyiA"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 【MultiHeadAttentionクラスの定義】"
      ],
      "metadata": {
        "id": "-kduGxZ6tdei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = x.view(batch_size, -1, self.num_heads, self.depth)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        query = self.W_q(query)\n",
        "        key = self.W_k(key)\n",
        "        value = self.W_v(value)\n",
        "\n",
        "        query = self.split_heads(query, batch_size)\n",
        "        key = self.split_heads(key, batch_size)\n",
        "        value = self.split_heads(value, batch_size)\n",
        "\n",
        "        attention_score = torch.matmul(query, key.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.depth, dtype=torch.float32))\n",
        "\n",
        "        if mask is not None:\n",
        "            attention_score += mask\n",
        "\n",
        "        attention_weights = F.softmax(attention_score, dim=-1)\n",
        "        attention = torch.matmul(attention_weights, value)\n",
        "        attention = attention.permute(0, 2, 1, 3).contiguous()\n",
        "        attention = attention.view(batch_size, -1, self.d_model)\n",
        "\n",
        "        return self.W_o(attention)\n"
      ],
      "metadata": {
        "id": "gD3mXhzMo1hn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiHeadAttentionの挙動確認"
      ],
      "metadata": {
        "id": "N3e7XbQMo3ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 疑似データ（Embedderによって1batch,7tokenが埋め込まれた，埋め込み次元数128のデータ）\n",
        "input_data = torch.randn(1, 7, 128)\n",
        "\n",
        "d_model = 128\n",
        "num_heads = 4  # ヘッド数を4に設定\n",
        "\n",
        "# Multi-Head Attentionモデルの作成\n",
        "multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "# テストデータをMulti-Head Attentionに流す\n",
        "output = multi_head_attention(input_data, input_data, input_data)\n",
        "\n",
        "# 出力のサイズを確認\n",
        "print(\"output.shape:\", output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMguJwbtSpCp",
        "outputId": "2252e806-5447-490c-cb06-c807eaaff930"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "output.shape: torch.Size([1, 7, 128])\n"
          ]
        }
      ]
    }
  ]
}